{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WXu1r8qvSzWf"},"source":["# Twin-Delayed DDPG\n","\n","Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."]},{"cell_type":"markdown","metadata":{"id":"YRzQUhuUTc0J"},"source":["## Installing the packages"]},{"cell_type":"code","metadata":{"id":"HAHMB0Ze8fU0","outputId":"189ba711-47f5-4498-f9ac-3c43eb6d43f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724658419793,"user_tz":-330,"elapsed":9145,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["!pip install pybullet\n","!pip install gym==0.22"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pybullet in /usr/local/lib/python3.10/dist-packages (3.2.6)\n","Requirement already satisfied: gym==0.22 in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (0.0.8)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xjm2onHdT-Av"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"Ikr2p0Js8iB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724658424873,"user_tz":-330,"elapsed":3462,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}},"outputId":"a70bc20a-30dd-4d10-9d71-554d100037d5"},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pybullet_envs\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gym import wrappers\n","from torch.autograd import Variable\n","from collections import deque\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y2nGdtlKVydr"},"source":["## Step 1: We initialize the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"u5rW0IDB8nTO","executionInfo":{"status":"ok","timestamp":1724658427891,"user_tz":-330,"elapsed":1040,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["class ReplayBuffer(object):\n","\n","  def __init__(self, max_size=1e6):\n","    self.storage = []\n","    self.max_size = max_size\n","    self.ptr = 0\n","\n","  def add(self, transition):\n","    if len(self.storage) == self.max_size:\n","      self.storage[int(self.ptr)] = transition\n","      self.ptr = (self.ptr + 1) % self.max_size\n","    else:\n","      self.storage.append(transition)\n","      self.ptr = (self.ptr + 1) % self.max_size\n","\n","  def sample(self, batch_size):\n","    ind = np.random.randint(0, len(self.storage), size=batch_size)\n","    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n","    for i in ind:\n","      state, next_state, action, reward, done = self.storage[i]\n","      batch_states.append(np.array(state, copy=False))\n","      batch_next_states.append(np.array(next_state, copy=False))\n","      batch_actions.append(np.array(action, copy=False))\n","      batch_rewards.append(np.array(reward, copy=False))\n","      batch_dones.append(np.array(done, copy=False))\n","    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jb7TTaHxWbQD"},"source":["## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"]},{"cell_type":"code","metadata":{"id":"4CeRW4D79HL0","executionInfo":{"status":"ok","timestamp":1724658429716,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["class Actor(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRDDce8FXef7"},"source":["## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"]},{"cell_type":"code","metadata":{"id":"OCee7gwR9Jrs","executionInfo":{"status":"ok","timestamp":1724658431642,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["class Critic(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzIDuONodenW"},"source":["## Steps 4 to 15: Training Process"]},{"cell_type":"code","metadata":{"id":"zzd0H1xukdKe","executionInfo":{"status":"ok","timestamp":1724658432576,"user_tz":-330,"elapsed":1,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","\n","    for it in range(iterations):\n","\n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","\n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","\n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","\n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","\n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","\n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","\n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","\n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","\n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","\n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka-ZRtQvjBex"},"source":["## We make a function that evaluates the policy by calculating its average reward over 10 episodes"]},{"cell_type":"code","metadata":{"id":"qabqiYdp9wDM","executionInfo":{"status":"ok","timestamp":1724658434969,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGuKmH_ijf7U"},"source":["## We set the parameters"]},{"cell_type":"code","metadata":{"id":"HFj6wbAo97lk","executionInfo":{"status":"ok","timestamp":1724658437102,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n","seed = 0 # Random seed number\n","start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n","eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n","max_timesteps = 5e5 # Total number of iterations/timesteps\n","save_models = True # Boolean checker whether or not to save the pre-trained model\n","expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n","batch_size = 100 # Size of the batch\n","discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n","tau = 0.005 # Target network update rate\n","policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n","noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n","policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hjwf2HCol3XP"},"source":["## We create a file name for the two saved models: the Actor and Critic models"]},{"cell_type":"code","metadata":{"id":"1fyH8N5z-o3o","outputId":"9421583a-9b6c-4d69-ccaf-1f1e762f949c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724658439740,"user_tz":-330,"elapsed":907,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"kop-C96Aml8O"},"source":["## We create a folder inside which will be saved the trained models"]},{"cell_type":"code","metadata":{"id":"Src07lvY-zXb","executionInfo":{"status":"ok","timestamp":1724658441770,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["if not os.path.exists(\"./results\"):\n","  os.makedirs(\"./results\")\n","if save_models and not os.path.exists(\"./pytorch_models\"):\n","  os.makedirs(\"./pytorch_models\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEAzOd47mv1Z"},"source":["## We create the PyBullet environment"]},{"cell_type":"code","metadata":{"id":"CyQXJUIs-6BV","executionInfo":{"status":"ok","timestamp":1724658443845,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["env = gym.make(env_name)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YdPG4HXnNsh"},"source":["## We set seeds and we get the necessary information on the states and actions in the chosen environment"]},{"cell_type":"code","metadata":{"id":"Z3RufYec_ADj","executionInfo":{"status":"ok","timestamp":1724658446562,"user_tz":-330,"elapsed":650,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWEgDAQxnbem"},"source":["## We create the policy network (the Actor model)"]},{"cell_type":"code","metadata":{"id":"wTVvG7F8_EWg","executionInfo":{"status":"ok","timestamp":1724658449547,"user_tz":-330,"elapsed":917,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["policy = TD3(state_dim, action_dim, max_action)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI60VN2Unklh"},"source":["[link text](https://)## We create the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"sd-ZsdXR_LgV","executionInfo":{"status":"ok","timestamp":1724658451768,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["replay_buffer = ReplayBuffer()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYOpCyiDnw7s"},"source":["## We define a list where all the evaluation results over 10 episodes are stored"]},{"cell_type":"code","metadata":{"id":"dhC_5XJ__Orp","outputId":"e3472368-1675-486d-d46d-058de82e4c4f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724658454728,"user_tz":-330,"elapsed":1113,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["evaluations = [evaluate_policy(policy)]"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Average Reward over the Evaluation Step: 9.807990\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"xm-4b3p6rglE"},"source":["## We create a new folder directory in which the final results (videos of the agent) will be populated"]},{"cell_type":"code","metadata":{"id":"MTL9uMd0ru03","executionInfo":{"status":"ok","timestamp":1724658457141,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["def mkdir(base, name):\n","    path = os.path.join(base, name)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    return path\n","work_dir = mkdir('exp', 'brs')\n","monitor_dir = mkdir(work_dir, 'monitor')\n","max_episode_steps = env._max_episode_steps\n","save_env_vid = False\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31n5eb03p-Fm"},"source":["## We initialize the variables"]},{"cell_type":"code","metadata":{"id":"1vN5EvxK_QhT","executionInfo":{"status":"ok","timestamp":1724658458994,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["total_timesteps = 0\n","timesteps_since_eval = 0\n","episode_num = 0\n","done = True\n","t0 = time.time()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9gsjvtPqLgT"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y_ouY4NH_Y0I","outputId":"fdd9cbfc-9d34-436d-894f-401da320e43e","colab":{"base_uri":"https://localhost:8080/","height":599},"executionInfo":{"status":"error","timestamp":1724658499263,"user_tz":-330,"elapsed":38404,"user":{"displayName":"Shah Akash","userId":"06197915452538287747"}}},"source":["max_timesteps = 500000\n","# We start the main loop over 500,000 timesteps\n","while total_timesteps < max_timesteps:\n","\n","  # If the episode is done\n","  if done:\n","\n","    # If we are not at the very beginning, we start the training process of the model\n","    if total_timesteps != 0:\n","      print(\"Total Timesteps: {} Episode Num: {} Episode Timesteps: {} Reward: {}\".format(total_timesteps, episode_num, episode_timesteps, episode_reward))\n","      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n","      print(\"Training done\")\n","\n","    # We evaluate the episode and we save the policy\n","    if timesteps_since_eval >= eval_freq:\n","      timesteps_since_eval %= eval_freq\n","      evaluations.append(evaluate_policy(policy))\n","      policy.save(file_name, directory=\"./pytorch_models\")\n","      np.save(\"./results/%s\" % (file_name), evaluations)\n","\n","    # When the training step is done, we reset the state of the environment\n","    obs = env.reset()\n","\n","    # Set the Done to False\n","    done = False\n","\n","    # Set rewards and episode timesteps to zero\n","    episode_reward = 0\n","    episode_timesteps = 0\n","    episode_num += 1\n","\n","  # Before 10000 timesteps, we play random actions\n","  if total_timesteps < start_timesteps:\n","    action = env.action_space.sample()\n","  else: # After 10000 timesteps, we switch to the model\n","    action = policy.select_action(np.array(obs))\n","    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n","    if expl_noise != 0:\n","      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n","\n","  # The agent performs the action in the environment, then reaches the next state and receives the reward\n","  new_obs, reward, done, _ = env.step(action)\n","\n","  # We check if the episode is done\n","  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n","\n","  # We increase the total reward\n","  episode_reward += reward\n","\n","  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n","  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n","\n","  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n","  obs = new_obs\n","  episode_timesteps += 1\n","  total_timesteps += 1\n","  timesteps_since_eval += 1\n","\n","# We add the last policy evaluation to our list of evaluations and we save our model\n","evaluations.append(evaluate_policy(policy))\n","if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n","np.save(\"./results/%s\" % (file_name), evaluations)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Timesteps: 1000 Episode Num: 1 Episode Timesteps: 1000 Reward: 216.0587284240209\n","Training done\n","Total Timesteps: 2000 Episode Num: 2 Episode Timesteps: 1000 Reward: 503.28315028735483\n","Training done\n","Total Timesteps: 3000 Episode Num: 3 Episode Timesteps: 1000 Reward: 460.2148993243171\n","Training done\n","Total Timesteps: 4000 Episode Num: 4 Episode Timesteps: 1000 Reward: 497.47346719143184\n","Training done\n","Total Timesteps: 5000 Episode Num: 5 Episode Timesteps: 1000 Reward: 516.5101784127751\n","Training done\n","---------------------------------------\n","Average Reward over the Evaluation Step: 0.294672\n","---------------------------------------\n","Total Timesteps: 6000 Episode Num: 6 Episode Timesteps: 1000 Reward: 494.186249106023\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-f20c88cdabb2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Timesteps: {} Episode Num: {} Episode Timesteps: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-43483ad8edc5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;31m# Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"wi6e2-_pu05e"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"oW4d1YAMqif1"},"source":["class Actor(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x\n","\n","class Critic(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1\n","\n","# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","\n","    for it in range(iterations):\n","\n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","\n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","\n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","\n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","\n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","\n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","\n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","\n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","\n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","\n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n","\n","def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward\n","\n","env_name = \"AntBulletEnv-v0\"\n","seed = 0\n","\n","file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")\n","\n","eval_episodes = 10\n","save_env_vid = True\n","env = gym.make(env_name)\n","max_episode_steps = env._max_episode_steps\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()\n","env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n","policy = TD3(state_dim, action_dim, max_action)\n","policy.load(file_name, './pytorch_models/')\n","_ = evaluate_policy(policy, eval_episodes=eval_episodes)"],"execution_count":null,"outputs":[]}]}